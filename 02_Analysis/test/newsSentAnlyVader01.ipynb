{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_and_combine_csv():\n",
    "    folder_path = r\"D:\\NLP_Proj\\bucket01\"\n",
    "    \"\"\"\n",
    "    Reads all CSV files from the given folder and combines them into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing all the data from the CSV files.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for file in all_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def handle_duplicates(df):\n",
    "    \"\"\"\n",
    "    Prints the number of duplicate entries in the DataFrame and removes them.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate entries: {duplicate_count}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_cleaned = df.drop_duplicates()\n",
    "    print(\"Duplicates removed.\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def convert_timestamp(df, timestamp_column):\n",
    "    \"\"\"\n",
    "    Converts a timestamp column to a standard datetime format.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the timestamp column.\n",
    "    timestamp_column (str): The name of the column with timestamp data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the timestamp column converted to datetime.\n",
    "    \"\"\"\n",
    "    df[timestamp_column] = pd.to_datetime(df[timestamp_column], errors='coerce')\n",
    "    print(f\"Timestamps converted to datetime format in column: '{timestamp_column}'\")\n",
    "    return df\n",
    "\n",
    "def combine_text_columns(df, title_column, description_column, new_column_name=\"combined_text\"):\n",
    "    \"\"\"\n",
    "    Combines the title and description columns into a single text column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the title and description columns.\n",
    "    title_column (str): The name of the title column.\n",
    "    description_column (str): The name of the description column.\n",
    "    new_column_name (str): The name of the new column to store the combined text.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the new combined text column.\n",
    "    \"\"\"\n",
    "    df[new_column_name] = df[title_column].astype(str) + \" \" + df[description_column].astype(str)\n",
    "    #print(f\"Columns '{title_column}' and '{description_column}' combined into '{new_column_name}'.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Counts missing values in a DataFrame, prints a summary, and removes rows with missing values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check and handle missing values.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A cleaned DataFrame with rows containing missing values removed.\n",
    "    \"\"\"\n",
    "    # Count missing values per column\n",
    "    missing_summary = df.isnull().sum()\n",
    "    total_missing = missing_summary.sum()\n",
    "    \n",
    "    print(\"Summary of Missing Values (Before Cleaning):\")\n",
    "    print(missing_summary)\n",
    "    print(f\"\\nTotal missing values: {total_missing}\")\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    df_cleaned = df.dropna()\n",
    "    print(f\"\\nRows with missing values removed. Remaining rows: {len(df_cleaned)}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "combined_df = read_and_combine_csv()\n",
    "cleaned_df = handle_duplicates(combined_df)\n",
    "cleaned_df = handle_missing_values(cleaned_df)\n",
    "cleaned_df = convert_timestamp(cleaned_df, \"timestamp\")\n",
    "cleaned_df = combine_text_columns(cleaned_df, \"title\", \"description\", \"text_for_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\rutvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_for_analysis</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trade setup for Wednesday: Top 15 things to kn...</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technical View: Bearish Belt Hold formation si...</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taking Stock: Market snaps two-day gains, clos...</td>\n",
       "      <td>-0.1063</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nifty, Sensex end flat amid lack of fresh trig...</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mid-day | Nifty, Sensex pare gains, trade lowe...</td>\n",
       "      <td>0.8591</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text_for_analysis  compound_score sentiment\n",
       "0  Trade setup for Wednesday: Top 15 things to kn...          0.7351  positive\n",
       "1  Technical View: Bearish Belt Hold formation si...          0.7096  positive\n",
       "2  Taking Stock: Market snaps two-day gains, clos...         -0.1063  negative\n",
       "3  Nifty, Sensex end flat amid lack of fresh trig...          0.8402  positive\n",
       "4  Mid-day | Nifty, Sensex pare gains, trade lowe...          0.8591  positive"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Ensure the VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER sentiment analysis\n",
    "cleaned_df['sentiment_scores'] = cleaned_df['text_for_analysis'].apply(lambda x: sia.polarity_scores(x))\n",
    "cleaned_df['compound_score'] = cleaned_df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "cleaned_df['sentiment'] = cleaned_df['compound_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "cleaned_df[['text_for_analysis', 'compound_score', 'sentiment']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as CSV to: D:\\NLP_Proj\\cleaned_data_nnn.csv\n"
     ]
    }
   ],
   "source": [
    "output_file_path = r\"D:\\NLP_Proj\\cleaned_data_nnn.csv\"\n",
    "\n",
    "cleaned_df.to_csv(output_file_path, index=False)\n",
    "print(f\"DataFrame saved as CSV to: {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
