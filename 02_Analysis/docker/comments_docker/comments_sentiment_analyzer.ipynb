{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import datetime\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "#-------------------------------Changes required-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def read_and_combine_csv_s3(bucket_name, folder_path, prefix_filter):\n",
    "    \"\"\"\n",
    "    Reads CSV files from a specified S3 bucket folder, sorts them name-wise in descending order,\n",
    "    processes the first file as daily data, and combines the first 7 files as weekly data.\n",
    "\n",
    "    Parameters:\n",
    "    bucket_name (str): The S3 bucket name.\n",
    "    folder_path (str): The folder path in the S3 bucket containing the CSV files.\n",
    "    prefix_filter (str): The prefix to filter the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - daily_df (pd.DataFrame): DataFrame of the first file (daily data).\n",
    "        - weekly_df (pd.DataFrame): DataFrame combining the first 7 files (weekly data).\n",
    "    \"\"\"\n",
    "    # Initialize S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Get the list of objects in the folder\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_path)\n",
    "\n",
    "    if 'Contents' not in response:\n",
    "        raise FileNotFoundError(\"No files found in the specified folder or bucket.\")\n",
    "    \n",
    "    # Extract and sort keys in descending order\n",
    "    object_keys = sorted(\n",
    "        [obj['Key'] for obj in response['Contents'] if obj['Key'].startswith(f\"{folder_path}{prefix_filter}\") and obj['Key'].endswith('.csv')],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    if not object_keys:\n",
    "        raise FileNotFoundError(f\"No files matching the prefix '{prefix_filter}' found in the specified folder.\")\n",
    "    \n",
    "    # Read the first file as daily_df\n",
    "    daily_file_key = object_keys[0]\n",
    "\n",
    "    print(f\"Reading daily file: {daily_file_key}\")\n",
    "\n",
    "    daily_obj = s3.get_object(Bucket=bucket_name, Key=daily_file_key)\n",
    "    daily_df = pd.read_csv(StringIO(daily_obj['Body'].read().decode('utf-8')))\n",
    "    \n",
    "    # Read the first 7 files as weekly_df\n",
    "    weekly_keys = object_keys[:7]\n",
    "\n",
    "    for key in weekly_keys:\n",
    "        print(f\"Reading weekly file: {key}\")\n",
    "\n",
    "    weekly_df = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(StringIO(s3.get_object(Bucket=bucket_name, Key=key)['Body'].read().decode('utf-8')))\n",
    "            for key in weekly_keys\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    return daily_df, weekly_df\n",
    "\n",
    "\n",
    "def read_and_combine_csv_comments():\n",
    "    \"\"\"\n",
    "    Reads CSV files from a folder, sorts them by name, and processes the first file as daily data \n",
    "    and the first 7 files as weekly data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - daily_df (pd.DataFrame): DataFrame of the first file (daily data).\n",
    "        - weekly_df (pd.DataFrame): DataFrame combining the first 7 files (weekly data).\n",
    "    \"\"\"\n",
    "    folder_path = r\"D:\\NLP_Proj\\bucket02\"\n",
    "    \n",
    "    # List and sort all CSV files in ascending order\n",
    "    all_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.csv')], reverse=True)\n",
    "    \n",
    "    if not all_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in the specified folder.\")\n",
    "    \n",
    "    # Read the first file as daily data\n",
    "    daily_file_path = os.path.join(folder_path, all_files[0])\n",
    "    daily_df = pd.read_csv(daily_file_path)\n",
    "\n",
    "    # Read the first 7 files as weekly data\n",
    "    weekly_files = all_files[:7]\n",
    "    weekly_df = pd.concat(\n",
    "        [pd.read_csv(os.path.join(folder_path, file)) for file in weekly_files],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    return daily_df, weekly_df\n",
    "\n",
    "#------------------------------------------------------------------------------------------No changes------------------------------------------------------------------------------------------------\n",
    "def handle_duplicates(df):\n",
    "    \"\"\"\n",
    "    Prints the number of duplicate entries in the DataFrame and removes them.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate entries: {duplicate_count}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_cleaned = df.drop_duplicates()\n",
    "    print(\"Duplicates removed.\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def convert_timestamp(df, timestamp_column):\n",
    "    \"\"\"\n",
    "    Converts a timestamp column to a standard datetime format.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the timestamp column.\n",
    "    timestamp_column (str): The name of the column with timestamp data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the timestamp column converted to datetime.\n",
    "    \"\"\"\n",
    "    df[timestamp_column] = pd.to_datetime(df[timestamp_column], errors='coerce')\n",
    "    print(f\"Timestamps converted to datetime format in column: '{timestamp_column}'\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Counts missing values in a DataFrame, prints a summary, and removes rows with missing values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check and handle missing values.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A cleaned DataFrame with rows containing missing values removed.\n",
    "    \"\"\"\n",
    "    # Count missing values per column\n",
    "    missing_summary = df.isnull().sum()\n",
    "    total_missing = missing_summary.sum()\n",
    "    \n",
    "    print(\"Summary of Missing Values (Before Cleaning):\")\n",
    "    print(missing_summary)\n",
    "    print(f\"\\nTotal missing values: {total_missing}\")\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    df_cleaned = df.dropna()\n",
    "    print(f\"\\nRows with missing values removed. Remaining rows: {len(df_cleaned)}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "def remove_stop_words_from_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Removes stop words from a specified column in a DataFrame using a stop words file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the text data.\n",
    "    column_name (str): The name of the column to process.\n",
    "    stop_words_path (str): The path to the file containing stop words (one per line).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the specified column cleaned of stop words.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words_path = r\"stop_words.txt\"\n",
    "\n",
    "    # Read the stop words from the file\n",
    "    with open(stop_words_path, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(word.strip() for word in file.readlines())\n",
    "    \n",
    "    # Define a function to remove stop words from text\n",
    "    def remove_stop_words(text):\n",
    "        text = re.sub(r'\\s+', ' ', str(text).strip())  # Ensure clean text input\n",
    "        return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    \n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"The DataFrame does not contain a column named '{column_name}'.\")\n",
    "    \n",
    "    # Apply the stop words removal function to the specified column\n",
    "    df[column_name] = df[column_name].astype(str).apply(remove_stop_words)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_column(df, column_name, language_model=\"en_core_web_sm\"):\n",
    "    \"\"\"\n",
    "    Lemmatizes the text in the specified column of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the text data.\n",
    "    column_name (str): The name of the column to lemmatize.\n",
    "    language_model (str): The spaCy language model to use (default: \"en_core_web_sm\").\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with lemmatized text in the specified column.\n",
    "    \"\"\"\n",
    "    # Load the spaCy language model\n",
    "    nlp = spacy.load(language_model)\n",
    "    \n",
    "    # Define a function to lemmatize text\n",
    "    def lemmatize_text(text):\n",
    "        doc = nlp(text)\n",
    "        return ' '.join(token.lemma_ for token in doc)\n",
    "    \n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"The DataFrame does not contain a column named '{column_name}'.\")\n",
    "    \n",
    "    # Apply the lemmatization function to the specified column\n",
    "    df[column_name] = df[column_name].astype(str).apply(lemmatize_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df = lemmatize_column(df, 'comment_content')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_comments(df):\n",
    "    \"\"\"\n",
    "    Cleans the comment_content column of a DataFrame by removing URLs, mentions, and hashtags.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the comments to clean.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with a cleaned comment_content column.\n",
    "    \"\"\"\n",
    "    # Define a cleaning function for text\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)    # Remove mentions\n",
    "        text = re.sub(r'#', '', text)      # Remove hashtag symbols\n",
    "        text = re.sub(r'[^\\w\\s\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF]', '', text)  # Remove special characters but retain emojis\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "        return text\n",
    "    \n",
    "    # Apply cleaning function to the comment_content column\n",
    "    if 'comment_content' in df.columns:\n",
    "        df['comment_content'] = df['comment_content'].astype(str).apply(clean_text)\n",
    "    else:\n",
    "        raise ValueError(\"The DataFrame does not contain a 'comment_content' column.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_vader_sentiment(df, column_name):\n",
    "    \"\"\"\n",
    "    Applies VADER sentiment analysis to the specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the text data.\n",
    "    column_name (str): The name of the column to analyze.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with additional columns for sentiment scores, compound score, and sentiment label.\n",
    "    \"\"\"\n",
    "    # Ensure the VADER lexicon is downloaded\n",
    "    try:\n",
    "        nltk.data.find('sentiment/vader_lexicon')\n",
    "    except LookupError:\n",
    "        nltk.download('vader_lexicon')\n",
    "\n",
    "    # Initialize the VADER Sentiment Analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Apply VADER sentiment analysis\n",
    "    df['sentiment_scores'] = df[column_name].apply(lambda x: sia.polarity_scores(x))\n",
    "    df['compound_score'] = df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "    df['sentiment'] = df['compound_score'].apply(\n",
    "        lambda x: 'positive' if x > 0.25 else ('negative' if x < -0.25 else 'neutral')\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_to_s3(df, bucket_name, full_path):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame as a CSV file to the specified S3 bucket with a timestamp in the filename.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to save.\n",
    "    bucket_name (str): The S3 bucket name.\n",
    "    folder_path (str): The folder path in the bucket where the file will be saved.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    csv_buffer = StringIO()\n",
    "    \n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Bucket=bucket_name, Key=full_path, Body=csv_buffer.getvalue())\n",
    "    \n",
    "    print(f\"DataFrame saved to S3: s3://{bucket_name}/{full_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading daily file: csv/nifty_forum_comments_20241204221258.csv\n",
      "Reading weekly file: csv/nifty_forum_comments_20241204221258.csv\n",
      "Reading weekly file: csv/nifty_forum_comments_20241204183955.csv\n",
      "Reading weekly file: csv/nifty_forum_comments_20241204091947.csv\n",
      "Reading weekly file: csv/nifty_forum_comments_20241204091454.csv\n",
      "Reading weekly file: csv/nifty_forum_comments_20241204085906.csv\n",
      "Reading weekly file: csv/nifty_forum_comments_20241204043315.csv\n",
      "Reading weekly file: csv/nifty_forum_comments_20241204042642.csv\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"news-scraped-data\"\n",
    "folder_path = \"csv/\"\n",
    "news_prefix = \"nifty_forum_comments_\"\n",
    "\n",
    "daily_df, weekly_df = read_and_combine_csv_s3(bucket_name, folder_path, news_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate entries: 2\n",
      "Duplicates removed.\n",
      "Summary of Missing Values (Before Cleaning):\n",
      "timestamp          0\n",
      "username           0\n",
      "comment_content    0\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 0\n",
      "\n",
      "Rows with missing values removed. Remaining rows: 176\n",
      "Timestamps converted to datetime format in column: 'timestamp'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/__7bnxx902z59ygv13sy_j780000gn/T/ipykernel_14925/3917191409.py:143: FutureWarning: Parsed string \"2024-12-04 22:56:03 IST\" included an un-recognized timezone \"IST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  df[timestamp_column] = pd.to_datetime(df[timestamp_column], errors='coerce')\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/praharmodi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#daily\n",
    "cleaned_df = handle_duplicates(daily_df)\n",
    "cleaned_df = handle_missing_values(cleaned_df)\n",
    "cleaned_df = convert_timestamp(cleaned_df, \"timestamp\")\n",
    "cleaned_df = clean_comments(cleaned_df)\n",
    "cleaned_df = remove_stop_words_from_column(cleaned_df,\"comment_content\")\n",
    "cleaned_df = lemmatize_column(cleaned_df,\"comment_content\")\n",
    "cleaned_df_daily = apply_vader_sentiment(cleaned_df, 'comment_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the current timestamp\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "output_file_path = fr\"daily_comments_{current_timestamp}.csv\"\n",
    "\n",
    "# cleaned_df_daily.to_csv(output_file_path, index=False)\n",
    "# print(f\"DataFrame saved as CSV to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to S3: s3://sentiment-analyzed-data/csv/daily_comments_20241205_120818.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the analyzed DataFrame\n",
    "output_bucket = \"sentiment-analyzed-data\"\n",
    "full_path = f\"csv/{output_file_path}\"\n",
    "save_to_s3(cleaned_df_daily, output_bucket, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate entries: 37\n",
      "Duplicates removed.\n",
      "Summary of Missing Values (Before Cleaning):\n",
      "timestamp          0\n",
      "username           0\n",
      "comment_content    0\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 0\n",
      "\n",
      "Rows with missing values removed. Remaining rows: 969\n",
      "Timestamps converted to datetime format in column: 'timestamp'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/__7bnxx902z59ygv13sy_j780000gn/T/ipykernel_14925/3917191409.py:143: FutureWarning: Parsed string \"2024-12-04 22:56:03 IST\" included an un-recognized timezone \"IST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  df[timestamp_column] = pd.to_datetime(df[timestamp_column], errors='coerce')\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/praharmodi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#weekly\n",
    "cleaned_df = handle_duplicates(weekly_df)\n",
    "cleaned_df = handle_missing_values(cleaned_df)\n",
    "cleaned_df = convert_timestamp(cleaned_df, \"timestamp\")\n",
    "cleaned_df = clean_comments(cleaned_df)\n",
    "cleaned_df = remove_stop_words_from_column(cleaned_df,\"comment_content\")\n",
    "cleaned_df = lemmatize_column(cleaned_df,\"comment_content\")\n",
    "cleaned_df_weekly = apply_vader_sentiment(cleaned_df, 'comment_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the current timestamp\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "output_file_path = fr\"weekly_comments_{current_timestamp}.csv\"\n",
    "\n",
    "# cleaned_df_weekly.to_csv(output_file_path, index=False)\n",
    "# print(f\"DataFrame saved as CSV to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to S3: s3://sentiment-analyzed-data/csv/weekly_comments_20241205_120821.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the analyzed DataFrame\n",
    "output_bucket = \"sentiment-analyzed-data\"\n",
    "full_path = f\"csv/{output_file_path}\"\n",
    "save_to_s3(cleaned_df_weekly, output_bucket, full_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
